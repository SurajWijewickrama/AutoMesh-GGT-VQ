{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gevc4t__bsbq",
        "outputId": "a391f0de-6176-4071-9b23-2998abeb0d90"
      },
      "outputs": [],
      "source": [
        "# # prompt: get the drive connected to the notebook\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.11\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip uninstall torch torchvision torchaudio torch-cluster torch-geometric -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.57.0-cp311-cp311-win_amd64.whl.metadata (104 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.8-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
            "Collecting numpy>=1.23 (from matplotlib)\n",
            "  Downloading numpy-2.2.4-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\studies\\finalyear\\fyp\\ggt\\.conda\\lib\\site-packages (from matplotlib) (24.2)\n",
            "Collecting pillow>=8 (from matplotlib)\n",
            "  Downloading pillow-11.1.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in d:\\studies\\finalyear\\fyp\\ggt\\.conda\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\studies\\finalyear\\fyp\\ggt\\.conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading matplotlib-3.10.1-cp311-cp311-win_amd64.whl (8.1 MB)\n",
            "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/8.1 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 1.0/8.1 MB 2.6 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 1.8/8.1 MB 3.1 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 2.9/8.1 MB 3.6 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 3.9/8.1 MB 4.0 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 4.5/8.1 MB 3.7 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 5.5/8.1 MB 3.9 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 6.0/8.1 MB 3.8 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 6.8/8.1 MB 3.8 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 7.6/8.1 MB 3.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.1/8.1 MB 3.6 MB/s eta 0:00:00\n",
            "Downloading contourpy-1.3.1-cp311-cp311-win_amd64.whl (219 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.57.0-cp311-cp311-win_amd64.whl (2.2 MB)\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
            "   -------------------------------------- - 2.1/2.2 MB 3.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.2/2.2 MB 3.2 MB/s eta 0:00:00\n",
            "Downloading kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
            "Downloading numpy-2.2.4-cp311-cp311-win_amd64.whl (12.9 MB)\n",
            "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.5/12.9 MB 3.4 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 1.3/12.9 MB 3.5 MB/s eta 0:00:04\n",
            "   ------ --------------------------------- 2.1/12.9 MB 3.7 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 2.9/12.9 MB 4.0 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 3.9/12.9 MB 3.9 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 4.7/12.9 MB 4.0 MB/s eta 0:00:03\n",
            "   ----------------- ---------------------- 5.5/12.9 MB 3.9 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 6.6/12.9 MB 4.0 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 7.3/12.9 MB 4.1 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 8.1/12.9 MB 4.1 MB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 9.2/12.9 MB 4.1 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 9.7/12.9 MB 4.0 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 10.7/12.9 MB 4.0 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.8/12.9 MB 4.1 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 12.6/12.9 MB 4.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  12.8/12.9 MB 4.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.9/12.9 MB 3.9 MB/s eta 0:00:00\n",
            "Downloading pillow-11.1.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
            "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
            "   ----------- ---------------------------- 0.8/2.6 MB 4.2 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 1.6/2.6 MB 4.2 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 2.4/2.6 MB 3.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.6/2.6 MB 3.6 MB/s eta 0:00:00\n",
            "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "Installing collected packages: pyparsing, pillow, numpy, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.57.0 kiwisolver-1.4.8 matplotlib-3.10.1 numpy-2.2.4 pillow-11.1.0 pyparsing-3.2.3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-win_amd64.whl (2449.4 MB)\n",
            "     --------- ------------------------------ 0.6/2.4 GB ? eta -:--:--0:12:22\n",
            "\n",
            "Error: ERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
            "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 98, in read\n",
            "    data: bytes = self.__fp.read(amt)\n",
            "                  ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\http\\client.py\", line 473, in read\n",
            "    s = self.fp.read(amt)\n",
            "        ^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\ssl.py\", line 1314, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\ssl.py\", line 1166, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TimeoutError: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 106, in _run_wrapper\n",
            "    status = _inner_run()\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 97, in _inner_run\n",
            "    return self.run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 386, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 95, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 546, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 397, in resolve\n",
            "    self._add_to_criteria(self.state.criteria, r, parent=None)\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 173, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 156, in __bool__\n",
            "    return bool(self._sequence)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 174, in __bool__\n",
            "    return any(self)\n",
            "           ^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 162, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 53, in _iter_built\n",
            "    candidate = func()\n",
            "                ^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 187, in _make_candidate_from_link\n",
            "    base: Optional[BaseCandidate] = self._make_base_candidate_from_link(\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 233, in _make_base_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "                                       ^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 304, in __init__\n",
            "    super().__init__(\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 159, in __init__\n",
            "    self.dist = self._prepare()\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 236, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 315, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 527, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 598, in _prepare_linked_requirement\n",
            "    local_file = unpack_url(\n",
            "                 ^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 170, in unpack_url\n",
            "    file = get_http_url(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 111, in get_http_url\n",
            "    from_path, content_type = download(link, temp_dir.path)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\network\\download.py\", line 148, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 55, in _rich_progress_bar\n",
            "    for chunk in iterable:\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 65, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
            "    with self._error_catcher():\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\contextlib.py\", line 158, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"d:\\Studies\\FinalYear\\FYP\\GGT\\.conda\\Lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='download.pytorch.org', port=443): Read timed out.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def run_command(command):\n",
        "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
        "    output, error = process.communicate()\n",
        "    return output.decode(), error.decode()\n",
        "\n",
        "# Install PyTorch with CUDA support\n",
        "cmd = f\"{sys.executable} -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\"\n",
        "output, error = run_command(cmd)\n",
        "print(\"Output:\", output)\n",
        "print(\"Error:\", error if error else \"No errors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_cluster-1.6.3%2Bpt25cu121-cp311-cp311-win_amd64.whl (1.6 MB)\n",
            "     ---------------------------------------- 1.6/1.6 MB 4.0 MB/s eta 0:00:00\n",
            "Collecting scipy (from torch-cluster)\n",
            "  Downloading scipy-1.15.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in d:\\studies\\finalyear\\fyp\\ggt\\.conda\\lib\\site-packages (from scipy->torch-cluster) (2.2.4)\n",
            "Downloading scipy-1.15.2-cp311-cp311-win_amd64.whl (41.2 MB)\n",
            "   ---------------------------------------- 41.2/41.2 MB 3.4 MB/s eta 0:00:00\n",
            "Installing collected packages: scipy, torch-cluster\n",
            "Successfully installed scipy-1.15.2 torch-cluster-1.6.3+pt25cu121\n",
            "\n",
            "Error: No errors\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def run_command(command):\n",
        "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
        "    output, error = process.communicate()\n",
        "    return output.decode(), error.decode()\n",
        "\n",
        "# Install torch-cluster for PyTorch 2.5.1+cu121\n",
        "cmd = f\"{sys.executable} -m pip install torch-cluster -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\"\n",
        "output, error = run_command(cmd)\n",
        "print(\"Output:\", output)\n",
        "print(\"Error:\", error if error else \"No errors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to import torch-cluster: No module named 'torch'\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from torch_cluster import knn  # Test import\n",
        "    print(\"torch-cluster installed successfully!\")\n",
        "except ImportError as e:\n",
        "    print(f\"Failed to import torch-cluster: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ1DvgVqOMvK",
        "outputId": "3263a192-2a44-407d-cf01-9ccd01e61942"
      },
      "outputs": [],
      "source": [
        "# !pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.__version__)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.version.cuda)  \u001b[38;5;66;03m# Should return the CUDA version if available\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)  # Should return the CUDA version if available\n",
        "print(torch.backends.cudnn.enabled)  # Should be True if CUDA is properly configured\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "if cuda version none - install pytorch with cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Installing and laoding cuda and othe packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is available. Using: NVIDIA GeForce GTX 1650\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available. Using:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHBi_C-sGbYG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "f14f5a8c778a4393933842e6737cb631",
            "7bb00053b88d47d684ded22fc79b7061",
            "6ec9249ba6364b8eb9c8cdee07d62f28",
            "b9b1310b2cc844b58d36f03ce2121df6",
            "0cd779ddf0be49bb857bbee186a8bdb5",
            "ab44d06347094ed5bc15e5ee232eac5f",
            "c2dcb84419e84f4c94b062e256c60fc7",
            "37ade2c5ac3e4fd38a02200c84bac548",
            "e8efdfafa7594d648715b7388115b6bc",
            "5ee3cfc2d2a64a5286907d9d1e1c5a13",
            "2397f1d2773d46ba8a3f408208293888",
            "97c440d79b63422bb1653f7f06a1a079",
            "b21fdc01b76b484f88ebff488171588a",
            "d56aa33f2cf843109608aad4a243c294",
            "c7ca3809b88b4a0b8668897452dbda7c",
            "2e9b5736d2414acda9d15ac96473c885",
            "51c8fafad03b49b993b1569ca396197b",
            "b628ae7a8f6544c286311ac4cae11eed",
            "bd21d9d2bfed40d8887ff97cbb571205",
            "97bdf1a4b7424908b96bd0454cd3a708",
            "d230aec0b9c540cc892ecd07012c404e",
            "134d780ed6c64778a4ce565339b61686",
            "e688fbce31824171994232209d5c43c3",
            "90e0bb2fe52e4080be9913569e83c314",
            "dbad3e0bf4104835abeb6966d9865d66",
            "607db37930064289878b1791b8ac7bd4",
            "b0658da51e9645af95ce49e50281cc6d",
            "6cbde44d11334b5b944c8410c8cb0d38",
            "f43b792d1ac142b4b29d3a6081b89646",
            "90fe37851c7a47138a7006eb54fcdd4d",
            "5971a7ddd8594a5cb9ec060168bc14ac",
            "0f0958f6189b4948926a236c26eb8c5f",
            "337eb33f480440a48149ce9c1a1aa4a6",
            "97c786a45441467b8d874fe5c25c6126",
            "cc7945f033b24db089f116da7ac98aeb",
            "9892b898f51540098e2fc72b6a41ebce",
            "d68069b8111e47b69365b3e3760d1edf",
            "29cb580877af43e586f4d76c288a6380",
            "5d15e1ab66e147eeb027ebf9d86bc435",
            "f1b416f44ddd4b4c9df5eef75978c6f4",
            "36355540013a4f78be3ca41e44b6926d",
            "a2a056a85c984797b34fa0fdb45a1298",
            "297b8c9d245f42a4864d853f0674aae6",
            "24027343e539405a95bb88f4cf0d5044",
            "97e43e3f2ff4430589ce42ac608b790c",
            "1093834e1ad84e52adc3155a979bc1a8",
            "9db512326f264814aa7c1172dc6c8ea3",
            "8002c62e95af4128b45b4759fd1ae9a7",
            "9de36e3c313a4a789a849204162e18dc",
            "6ebf19089e0f460aaa211c6ff1996b02",
            "bab982ff160a4d73b993ed5db2c49218",
            "382bc7f77d0d47f9ae4bbb00a40128e5",
            "5bbb1a78cf5b49feb35d9f1a3a55a8db",
            "f9a0986b39f6486f8a1ebcc597d744ba",
            "2e6617cbf36c4e91973b888cd244bf2b"
          ]
        },
        "id": "wVM39ezfKKpV",
        "outputId": "d86d5aff-67fa-4d95-aa20-68572ffa8d4a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'transformers'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertModel\n\u001b[32m      3\u001b[39m tokenizer = BertTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m bert_model = BertModel.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "def preprocess_object(obj, caption_text, tokenizer, device):\n",
        "    vertices = torch.tensor(obj['v'], dtype=torch.float, device=device)\n",
        "    edges = torch.tensor(obj['e'], dtype=torch.long, device=device).t()\n",
        "\n",
        "    # Tokenize the caption text.\n",
        "    inputs = tokenizer(caption_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
        "    # Move tokenized inputs to device.\n",
        "    input_ids = inputs[\"input_ids\"].to(device=device, dtype=torch.long)\n",
        "    attention_mask = inputs.get(\"attention_mask\", None)\n",
        "    if attention_mask is not None:\n",
        "        attention_mask = attention_mask.to(device=device, dtype=torch.long)\n",
        "\n",
        "    # Instead of computing embeddings with torch.no_grad(), we store the tokenized text.\n",
        "    text_label = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "    \n",
        "    return Data(x=vertices, edge_index=edges, text_label=text_label)\n",
        "\n",
        "def load_json_and_caption(json_file_path, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Loads the JSON data from json_file_path and locates the corresponding caption file.\n",
        "    Returns a graph Data object.\n",
        "    \"\"\"\n",
        "    # Read the JSON file\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        obj = json.load(f)\n",
        "\n",
        "    # Derive the base name from the JSON filename (e.g., 'model_id' from 'model_id.json')\n",
        "    base_name = os.path.splitext(os.path.basename(json_file_path))[0]\n",
        "    folder_path = os.path.dirname(json_file_path)\n",
        "\n",
        "    # Attempt to find a caption file in the same folder (e.g., 'model_id_45_caption.txt')\n",
        "    caption_text = \"\"\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        if file_name.startswith(base_name) and file_name.endswith(\"_caption.txt\"):\n",
        "            caption_file_path = os.path.join(folder_path, file_name)\n",
        "            with open(caption_file_path, 'r', encoding='utf-8') as cf:\n",
        "                caption_text = cf.read().strip()\n",
        "            break\n",
        "            \n",
        "    # Fallback: if no caption file is found, use obj['n'] if available\n",
        "    if not caption_text:\n",
        "        print(f\"No matching caption file found for {base_name}, using obj['n'] if available.\")\n",
        "        caption_text = obj.get('n', '')\n",
        "\n",
        "    return preprocess_object(obj, caption_text, tokenizer, device)\n",
        "\n",
        "def create_dataset(root_folder, tokenizer, device, max_nodes=2500):\n",
        "    \"\"\"\n",
        "    Iterates through subfolders in the root folder, loads JSON mesh data and caption files,\n",
        "    and filters out any data points with more than max_nodes nodes.\n",
        "    \n",
        "    The folder structure is assumed to be:\n",
        "      root_folder/\n",
        "          model_id/\n",
        "              model_id.json\n",
        "              model_id_45_caption.txt\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "\n",
        "    # List all items in the root folder\n",
        "    for model_id in os.listdir(root_folder):\n",
        "        model_folder = os.path.join(root_folder, model_id)\n",
        "        if os.path.isdir(model_folder):\n",
        "            json_filename = f\"{model_id}.json\"\n",
        "            json_path = os.path.join(model_folder, json_filename)\n",
        "            \n",
        "            if os.path.exists(json_path):\n",
        "                data = load_json_and_caption(json_path, tokenizer, device)\n",
        "                \n",
        "                # Filter out data with more than max_nodes nodes\n",
        "                num_nodes = data.x.size(0)\n",
        "                print()\n",
        "                if num_nodes > max_nodes:\n",
        "                    print(f\"Skipping {json_path} due to excessive nodes: {num_nodes} (max allowed {max_nodes}).\")\n",
        "                else:\n",
        "                    dataset.append(data)\n",
        "                    print(f\"Loaded data from {json_path} with {num_nodes} nodes.\")\n",
        "            else:\n",
        "                print(f\"No JSON file found for {model_id} in {model_folder}. Skipping.\")\n",
        "        else:\n",
        "            print(f\"{model_id} is not a directory. Skipping.\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Example usage:\n",
        "# Make sure to initialize your tokenizer and BERT model appropriately:\n",
        "# from transformers import BertTokenizer, BertModel\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#\n",
        "# root_folder = \"path/to/processed_models\"\n",
        "# dataset = create_dataset(root_folder, tokenizer, device, max_nodes=5000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_dataset_by_node_count(dataset, max_nodes=2500):\n",
        "    \"\"\"\n",
        "    Filters the dataset to remove data points with more than max_nodes and returns the\n",
        "    filtered dataset along with the minimum and maximum node counts.\n",
        "\n",
        "    Args:\n",
        "        dataset (list): List of PyG Data objects.\n",
        "        max_nodes (int): Maximum allowed number of nodes.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - filtered_dataset (list): Data points with at most max_nodes nodes.\n",
        "            - min_node_count (int or None): Minimum node count among filtered data, or None if empty.\n",
        "            - max_node_count (int or None): Maximum node count among filtered data, or None if empty.\n",
        "    \"\"\"\n",
        "    filtered_dataset = [data for data in dataset if data.x.size(0) <= max_nodes and data.x.size(0) > 700]\n",
        "    \n",
        "    if filtered_dataset:\n",
        "        node_counts = [data.x.size(0) for data in filtered_dataset]\n",
        "        min_node_count = min(node_counts)\n",
        "        max_node_count = max(node_counts)\n",
        "    else:\n",
        "        min_node_count, max_node_count = None, None\n",
        "\n",
        "    return filtered_dataset, min_node_count, max_node_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uCMzSSeFYPA"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def load_data_and_split(folder_path,tokenizer,device ):\n",
        "    dataset = create_dataset(folder_path,tokenizer,device)\n",
        "    filtered_dataset, min, max = filter_dataset_by_node_count(dataset)\n",
        "    min_node_count = min\n",
        "    max_node_count = max\n",
        "    train_size = int(0.8 * len(filtered_dataset))\n",
        "    val_size = int(0.1 * len(filtered_dataset))\n",
        "    test_size = len(filtered_dataset) - train_size - val_size\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        filtered_dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "    return train_dataset, val_dataset, test_dataset, min_node_count, max_node_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkzVXC5UWmhk",
        "outputId": "21d50b42-85ca-401b-998b-60fcdc3d00f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "C:\\Users\\Suraj\\AppData\\Local\\Temp\\ipykernel_5512\\3896733699.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
            "  train_dataset, val_dataset, test_dataset, min_node_count, max_node_count = load_data_and_split('D:\\Studies\\FinalYear\\FYP\\GGT\\sampleImages',tokenizer,device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded data from D:\\Studies\\FinalYear\\FYP\\GGT\\sampleImages\\1a04e3eab45ca15dd86060f189eb133\\1a04e3eab45ca15dd86060f189eb133.json with 1021 nodes.\n",
            "\n",
            "Skipping D:\\Studies\\FinalYear\\FYP\\GGT\\sampleImages\\1a6ad7a24bb89733f412783097373bdc\\1a6ad7a24bb89733f412783097373bdc.json due to excessive nodes: 8027 (max allowed 2500).\n",
            "\n",
            "Skipping D:\\Studies\\FinalYear\\FYP\\GGT\\sampleImages\\1a74b169a76e651ebc0909d98a1ff2b4\\1a74b169a76e651ebc0909d98a1ff2b4.json due to excessive nodes: 3371 (max allowed 2500).\n",
            "\n",
            "Loaded data from D:\\Studies\\FinalYear\\FYP\\GGT\\sampleImages\\1af4b32eafffb0f7ee60c37cbf99c1c\\1af4b32eafffb0f7ee60c37cbf99c1c.json with 940 nodes.\n",
            "\n",
            "Skipping D:\\Studies\\FinalYear\\FYP\\GGT\\sampleImages\\1bdeb4aaa0aaea4b4f95630cc18536e0\\1bdeb4aaa0aaea4b4f95630cc18536e0.json due to excessive nodes: 3149 (max allowed 2500).\n"
          ]
        }
      ],
      "source": [
        "# train_dataset, val_dataset, test_dataset, min_node_count, max_node_count = load_data_and_split('E:\\Danilka\\AutoMesh\\preprocessed-airplane-captions-sample\\captioned_models',tokenizer,device)\n",
        "train_dataset, val_dataset, test_dataset, min_node_count, max_node_count = load_data_and_split('D:\\Studies\\FinalYear\\FYP\\GGT\\sampleImages',tokenizer,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "940 minimum\n",
            "1021 maximum\n",
            "Original dataset size: 1\n"
          ]
        }
      ],
      "source": [
        "print(min_node_count , \"minimum\")\n",
        "print(max_node_count , \"maximum\")\n",
        "# Example usage:\n",
        "print(f\"Original dataset size: {len(train_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpl_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmplot3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axes3D\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Function to visualize the 3D graph\u001b[39;00m\n",
            "File \u001b[1;32md:\\SURAJ\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:1296\u001b[0m\n\u001b[0;32m   1292\u001b[0m     rcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend_fallback\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPLBACKEND\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m-> 1296\u001b[0m     rcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPLBACKEND\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_backend\u001b[39m(\u001b[38;5;241m*\u001b[39m, auto_select\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m    Return the name of the current backend.\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;124;03m    matplotlib.use\u001b[39;00m\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "File \u001b[1;32md:\\SURAJ\\anaconda3\\Lib\\site-packages\\matplotlib\\__init__.py:771\u001b[0m, in \u001b[0;36mRcParams.__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m    769\u001b[0m         cval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate[key](val)\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[1;32m--> 771\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set(key, cval)\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[1;31mValueError\u001b[0m: Key backend: 'module://matplotlib_inline.backend_inline' is not a valid value for backend; supported values are ['gtk3agg', 'gtk3cairo', 'gtk4agg', 'gtk4cairo', 'macosx', 'nbagg', 'notebook', 'qtagg', 'qtcairo', 'qt5agg', 'qt5cairo', 'tkagg', 'tkcairo', 'webagg', 'wx', 'wxagg', 'wxcairo', 'agg', 'cairo', 'pdf', 'pgf', 'ps', 'svg', 'template']"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Function to visualize the 3D graph\n",
        "def visualize_3d(vertices, edges):\n",
        "    \"\"\"\n",
        "    Visualize a 3D graph given vertices and edges.\n",
        "\n",
        "    :param vertices: numpy array of shape (n, 3) representing the 3D coordinates of the vertices.\n",
        "    :param edges: numpy array of shape (2, m) where each column represents an edge (start_index, end_index).\n",
        "    \"\"\"\n",
        "    # Create a 3D plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Ensure vertices are a numpy array\n",
        "    vertices = np.array(vertices)\n",
        "\n",
        "    # Plot vertices\n",
        "    ax.scatter(vertices[:, 0], vertices[:, 1], vertices[:, 2], c='b', marker='o', s=10, label=\"Vertices\")\n",
        "\n",
        "    # Plot edges\n",
        "    for i in range(edges.shape[1]):\n",
        "        v1, v2 = edges[:, i]  # Each column represents an edge (start_index, end_index)\n",
        "        ax.plot([vertices[v1, 0], vertices[v2, 0]],\n",
        "                [vertices[v1, 1], vertices[v2, 1]],\n",
        "                [vertices[v1, 2], vertices[v2, 2]], c='r', linewidth=1)\n",
        "\n",
        "    # Set plot labels\n",
        "    ax.set_xlabel('X Axis')\n",
        "    ax.set_ylabel('Y Axis')\n",
        "    ax.set_zlabel('Z Axis')\n",
        "    ax.legend()\n",
        "\n",
        "    # Set equal scaling for all axes\n",
        "    max_range = np.array([vertices[:, 0].max() - vertices[:, 0].min(),\n",
        "                          vertices[:, 1].max() - vertices[:, 1].min(),\n",
        "                          vertices[:, 2].max() - vertices[:, 2].min()]).max() / 2.0\n",
        "\n",
        "    mid_x = (vertices[:, 0].max() + vertices[:, 0].min()) * 0.5\n",
        "    mid_y = (vertices[:, 1].max() + vertices[:, 1].min()) * 0.5\n",
        "    mid_z = (vertices[:, 2].max() + vertices[:, 2].min()) * 0.5\n",
        "    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n",
        "    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n",
        "    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n",
        "\n",
        "# Extract vertices and edges from the dataset\n",
        "vertices_np, edges_np, text_label = train_dataset[1].x.cpu().numpy(), train_dataset[1].edge_index.cpu().numpy(),train_dataset[1].text_label\n",
        "\n",
        "# Print the first graph's data for verification\n",
        "print(\"Vertices:\", vertices_np.size)\n",
        "print(\"Edges:\", edges_np.size)\n",
        "print(\"Text:\", text_label)\n",
        "\n",
        "# Visualize the first graph\n",
        "visualize_3d(vertices_np, edges_np)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NodeCountPredictor(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(NodeCountPredictor, self).__init__()\n",
        "        self.node_count_predictor = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),  # Predict a single scalar\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            z: Latent representation [batch_size, latent_dim]\n",
        "        Returns:\n",
        "            predicted_node_count: Scalar prediction for the number of nodes\n",
        "        \"\"\"\n",
        "        return self.node_count_predictor(z).squeeze(-1)\n",
        "\n",
        "def pretrain_node_count_predictor(predictor, data_loader, optimizer, device, num_epochs=10):\n",
        "    predictor.train()\n",
        "    loss_fn = nn.MSELoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            z = data.latent_vector  # Use precomputed latent vector\n",
        "            true_num_nodes = data.num_nodes  # Ground truth node count\n",
        "            \n",
        "            # Forward pass\n",
        "            predicted_num_nodes = predictor(z)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_fn(predicted_num_nodes, true_num_nodes.float())\n",
        "            \n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(data_loader):.4f}\")\n",
        "\n",
        "\n",
        "# NodeCountPredictor()\n",
        "# pretrain_node_count_predictor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class BertTextEncoder(nn.Module):\n",
        "#     def __init__(self, bert_model_name=\"bert-base-uncased\", force_bert=False, freeze_layers=8):\n",
        "#         super(BertTextEncoder, self).__init__()\n",
        "#         self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "#         self.hidden_dim = self.bert.config.hidden_size\n",
        "#         self.force_bert = force_bert\n",
        "\n",
        "#         # Freeze embeddings and lower layers\n",
        "#         for param in self.bert.embeddings.parameters():\n",
        "#             param.requires_grad = False\n",
        "#         # Freeze the first freeze_layers encoder layers\n",
        "#         for layer in self.bert.encoder.layer[:freeze_layers]:\n",
        "#             for param in layer.parameters():\n",
        "#                 param.requires_grad = False\n",
        "\n",
        "#     def forward(self, text_input):\n",
        "#         if isinstance(text_input, dict):\n",
        "#             input_ids = text_input[\"input_ids\"]\n",
        "#             attention_mask = text_input.get(\"attention_mask\", None)\n",
        "#             print(\"attension\")\n",
        "#         else:\n",
        "#             input_ids = text_input\n",
        "#             attention_mask = None\n",
        "\n",
        "#         if input_ids.dim() == 3 and input_ids.size(-1) == self.hidden_dim and not self.force_bert:\n",
        "#             return input_ids\n",
        "\n",
        "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "#         embeddings = outputs.last_hidden_state\n",
        "#         return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DistilBertModel\n",
        "\n",
        "class BertTextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=\"distilbert-base-uncased\", force_bert=False):\n",
        "        super(BertTextEncoder, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
        "        self.hidden_dim = self.bert.config.dim  # DistilBERT uses 'dim' instead of 'hidden_size'\n",
        "        self.force_bert = force_bert\n",
        "\n",
        "    def forward(self, text_input):\n",
        "        if isinstance(text_input, dict):\n",
        "            input_ids = text_input[\"input_ids\"]\n",
        "            attention_mask = text_input.get(\"attention_mask\", None)\n",
        "        else:\n",
        "            input_ids = text_input\n",
        "            attention_mask = None\n",
        "\n",
        "        if input_ids.dim() == 3 and input_ids.size(-1) == self.hidden_dim and not self.force_bert:\n",
        "            return input_ids\n",
        "\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embeddings = outputs.last_hidden_state\n",
        "        return embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIKuwwe9fOMw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "\n",
        "# \"text_encoder\": TransformerTextEncoder(embed_dim=768, num_heads=4, num_layers=2).to(device),\n",
        "    # \"graph_encoder\": GraphEncoder(in_channels=3, hidden_channels=32, out_channels=64, num_layers=3).to(device),\n",
        "    # \"feature_fusion\": FeatureFusion(text_dim=768, graph_dim=64, fused_dim=128).to(device),\n",
        "\n",
        "# Transformer Encoder\n",
        "class TransformerTextEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_layers):\n",
        "        super(TransformerTextEncoder, self).__init__()\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "\n",
        "    def forward(self, text_input):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text_input: Tensor of shape [batch_size, seq_len, embed_dim]\n",
        "        Returns:\n",
        "            encoded_text: Tensor of shape [batch_size, seq_len, embed_dim]\n",
        "        \"\"\"\n",
        "        # Transpose to match Transformer input format: [seq_len, batch_size, embed_dim]\n",
        "        text_input = text_input.permute(1, 0, 2)\n",
        "\n",
        "        # Encode text\n",
        "        encoded_text = self.transformer(text_input)  # Shape: [seq_len, batch_size, embed_dim]\n",
        "\n",
        "        # Transpose back to [batch_size, seq_len, embed_dim]\n",
        "        return encoded_text.permute(1, 0, 2)\n",
        "\n",
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=3, hidden_channels=32, out_channels=64):\n",
        "        super(GraphEncoder, self).__init__()\n",
        "        self.layer1 = GCNConv(in_channels, hidden_channels)  # 3 -> 32\n",
        "        self.layer2 = GCNConv(hidden_channels, hidden_channels)  # 32 -> 32\n",
        "        self.layer3 = GCNConv(hidden_channels, out_channels)  # 32 -> 64\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.layer1(x, edge_index))\n",
        "        x = F.relu(self.layer2(x, edge_index))\n",
        "        x = self.layer3(x, edge_index)  # No ReLU on final layer\n",
        "        return x.mean(dim=0)  # Pooling to get latent feature z_G\n",
        "\n",
        "# Feature Fusion\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FeatureFusion(nn.Module):\n",
        "    def __init__(self, text_dim, graph_dim, fused_dim):\n",
        "     \n",
        "        super(FeatureFusion, self).__init__()\n",
        "        self.fc1 = nn.Linear(text_dim + graph_dim, 512)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, fused_dim)\n",
        "\n",
        "    def forward(self, c_text, z_G):\n",
        "      \n",
        "        c_text_pooled = c_text.mean(dim=1)  # Shape: [batch_size, text_dim]\n",
        "\n",
        "        # Ensure z_G is at least 2D (batch_size, graph_dim)\n",
        "        if z_G.ndim == 1:\n",
        "            z_G = z_G.unsqueeze(0).expand(c_text_pooled.size(0), -1)\n",
        "\n",
        "        # Concatenate features from text and graph\n",
        "        combined_features = torch.cat([c_text_pooled, z_G], dim=-1)  # Shape: [batch_size, text_dim + graph_dim]\n",
        "\n",
        "        # Pass through the first fully connected layer and activation (bottleneck)\n",
        "        hidden_features = self.activation(self.fc1(combined_features))  # Shape: [batch_size, hidden_dim]\n",
        "\n",
        "        # Pass through the final fully connected layer to produce fused features\n",
        "        fused_features = self.fc2(hidden_features)  # Shape: [batch_size, fused_dim]\n",
        "        return fused_features\n",
        "\n",
        "\n",
        "\n",
        "class VectorQuantization(nn.Module):\n",
        "    def __init__(self, codebook_size, embedding_dim, beta=1):\n",
        "        super().__init__()\n",
        "        self.codebook_size = codebook_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.beta = beta\n",
        "        self.codebook = nn.Embedding(codebook_size, embedding_dim)\n",
        "        self.codebook.weight.data.uniform_(-1 / codebook_size, 1 / codebook_size)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z_flattened = z.view(-1, z.size(-1))\n",
        "        distances = (torch.sum(z_flattened**2, dim=1, keepdim=True)\n",
        "                     - 2 * torch.matmul(z_flattened, self.codebook.weight.T)\n",
        "                     + torch.sum(self.codebook.weight**2, dim=1))\n",
        "        encoding_indices = torch.argmin(distances, dim=1)\n",
        "        z_q = self.codebook(encoding_indices).view(z.shape)\n",
        "        # Commitment loss\n",
        "        commit_loss = F.mse_loss(z, z_q.detach())\n",
        "        # Codebook loss\n",
        "        codebook_loss = F.mse_loss(z_q, z.detach())\n",
        "        loss = commit_loss + self.beta * codebook_loss\n",
        "        z_q = z + (z_q - z).detach()  # Straight-through estimator\n",
        "        return z_q, encoding_indices, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graph Decoder \n",
        "-> taking VQ and making node features and adjacent matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.nn import knn_graph\n",
        "\n",
        "class EdgeDecoder(nn.Module):\n",
        "    def __init__(self, node_dim, hidden_dim=64, k=4):\n",
        "        super().__init__()\n",
        "        self.k = k  \n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(2 * node_dim, hidden_dim),\n",
        "            nn.Dropout(0.5),  \n",
        "            nn.Softmax(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, node_embeddings):\n",
        "        num_nodes = node_embeddings.size(0)\n",
        "        # Use k-NN to get candidate edges\n",
        "        edge_index = knn_graph(node_embeddings, k=self.k, loop=False)\n",
        "        node_i = node_embeddings[edge_index[0]]\n",
        "        node_j = node_embeddings[edge_index[1]]\n",
        "        pairwise_input = torch.cat([node_i, node_j], dim=-1)\n",
        "        edge_logits = self.mlp(pairwise_input)\n",
        "        adj = torch.zeros(num_nodes, num_nodes, device=node_embeddings.device)\n",
        "        adj[edge_index[0], edge_index[1]] = torch.sigmoid(edge_logits).squeeze()\n",
        "        adj = adj + adj.T  # Ensure symmetry\n",
        "        return adj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "        # self.min_node_count = min_node_count\n",
        "        # self.max_node_count = max_node_count\n",
        "class GraphDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim, node_dim, edge_hidden_dim=64, min_node_count=50, max_node_count=10000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.node_decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.Softmax(),\n",
        "            nn.Dropout(0.5),  # Added dropout to prevent collapse\n",
        "            nn.Linear(64, node_dim)\n",
        "        )\n",
        "        # self.node_count_predictor = nn.Sequential(\n",
        "        #     nn.Linear(latent_dim, 32),\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.Linear(32, 1),\n",
        "        #     nn.ReLU()\n",
        "        # )\n",
        "        # mid_point = min_node_count + (max_node_count - min_node_count) / 2\n",
        "        # self.node_count_predictor[2].bias.data.fill_(mid_point)\n",
        "        self.edge_decoder = EdgeDecoder(node_dim, hidden_dim=edge_hidden_dim)\n",
        "        self.noise_factor = 0.5 # Noise factor for node decoding\n",
        "    def forward(self, z, true_num_nodes=None):\n",
        "        batch_size = z.size(0)\n",
        "        assert batch_size == 1, \"Currently, this decoder supports only batch_size=1.\"\n",
        "        # raw_node_counts = self.node_count_predictor(z).squeeze(-1)  # [batch_size]\n",
        "        # predicted_num_nodes = torch.clamp(raw_node_counts.round(), min=self.min_node_count, max=self.max_node_count).long()  # Keep as tensor\n",
        "        # num_nodes = true_num_nodes if true_num_nodes is not None else predicted_num_nodes.item()  # Use .item() only here\n",
        "        num_nodes = true_num_nodes.item()\n",
        "        noise = torch.randn(num_nodes, z.size(1), device=z.device) * self.noise_factor\n",
        "        z_repeated = z.repeat(num_nodes, 1) + noise\n",
        "        reconstructed_nodes = self.node_decoder(z_repeated)\n",
        "        reconstructed_adj = self.edge_decoder(reconstructed_nodes)\n",
        "        # return reconstructed_nodes, reconstructed_adj, predicted_num_nodes, raw_node_counts \n",
        "        return reconstructed_nodes, reconstructed_adj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LSTM to generate FF output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMFeatureFusion(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, fused_dim):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embed_dim (int): Dimension of the input text embeddings.\n",
        "            hidden_dim (int): Hidden dimension for both LSTM layers.\n",
        "            fused_dim (int): Dimension of the final fused output.\n",
        "        \"\"\"\n",
        "        super(LSTMFeatureFusion, self).__init__()\n",
        "        # First LSTM: takes full-dimensional input\n",
        "        self.lstm1 = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        # Second LSTM: takes the output of the first LSTM as input\n",
        "        self.lstm2 = nn.LSTM(input_size=hidden_dim, hidden_size=256, batch_first=True)\n",
        "        # Final fully connected layer to map hidden state to fused dimension\n",
        "        self.fc_out = nn.Linear(256, fused_dim)\n",
        "\n",
        "    def forward(self, text_embeddings):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text_embeddings: Tensor of shape [batch_size, seq_len, embed_dim]\n",
        "        Returns:\n",
        "            fused_prediction: Tensor of shape [batch_size, fused_dim]\n",
        "        \"\"\"\n",
        "        lstm_out1, _ = self.lstm1(text_embeddings)  # [batch_size, seq_len, hidden_dim]\n",
        "        lstm_out2, _ = self.lstm2(lstm_out1)          # [batch_size, seq_len, hidden_dim]\n",
        "        # Use the last time-step output from the second LSTM\n",
        "        last_hidden = lstm_out2[:, -1, :]             # [batch_size, hidden_dim]\n",
        "        fused_prediction = self.fc_out(last_hidden)   # [batch_size, fused_dim]\n",
        "        return fused_prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = {\n",
        "    \"text_encoder\": BertTextEncoder().to(device),\n",
        "    \"graph_encoder\": GraphEncoder(in_channels=3, hidden_channels=32, out_channels=64).to(device),\n",
        "    \"feature_fusion\": FeatureFusion(text_dim=768, graph_dim=64, fused_dim=128).to(device),\n",
        "    \"vector_quantizer\": VectorQuantization(codebook_size=512, embedding_dim=128).to(device),\n",
        "    \"graph_decoder\": GraphDecoder(latent_dim=128, node_dim=3, min_node_count=min_node_count, max_node_count=max_node_count).to(device),\n",
        "    \"lstm_feature_fusion\": LSTMFeatureFusion(embed_dim=768, hidden_dim=512, fused_dim=128).to(device)\n",
        "}\n",
        "# model = {\n",
        "#     \"text_encoder\": TransformerTextEncoder(embed_dim=768, num_heads=4, num_layers=2).to(device),\n",
        "#     \"graph_encoder\": GraphEncoder(in_channels=3, hidden_channels=32, out_channels=64).to(device),\n",
        "#     \"feature_fusion\": FeatureFusion(text_dim=768, graph_dim=64, fused_dim=128).to(device),\n",
        "#     \"vector_quantizer\": VectorQuantization(codebook_size=512, embedding_dim=128).to(device),\n",
        "#     \"graph_decoder\": GraphDecoder(latent_dim=128, node_dim=3, min_node_count=min_node_count, max_node_count=max_node_count).to(device),\n",
        "#     \"lstm_feature_fusion\": LSTMFeatureFusion(embed_dim=768, hidden_dim=512, fused_dim=128).to(device)\n",
        "# }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading Libaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loss functions and support def"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_mode(model, mode=\"train\"):\n",
        "    \"\"\"\n",
        "    Set all components of the model to train or eval mode.\n",
        "    Args:\n",
        "        model (dict): Dictionary of model components.\n",
        "        mode (str): \"train\" or \"eval\".\n",
        "    \"\"\"\n",
        "    for component in model.values():\n",
        "        if mode == \"train\":\n",
        "            component.train()\n",
        "        elif mode == \"eval\":\n",
        "            component.eval()\n",
        "\n",
        "# Training Function\n",
        "def pad_tensor(tensor, target_shape, padding_value=0.0):\n",
        "    \"\"\"\n",
        "    Pads or truncates a tensor to the target shape.\n",
        "    Args:\n",
        "        tensor (torch.Tensor): Input tensor.\n",
        "        target_shape (tuple): Desired shape after padding.\n",
        "        padding_value (float): Value for padding.\n",
        "    Returns:\n",
        "        torch.Tensor: Padded/truncated tensor.\n",
        "    \"\"\"\n",
        "    padded_tensor = torch.full(target_shape, padding_value, device=tensor.device, dtype=tensor.dtype)\n",
        "\n",
        "    # Calculate slices for assignment\n",
        "    slices = tuple(slice(0, min(s, t)) for s, t in zip(tensor.shape, target_shape))\n",
        "    padded_tensor[slices] = tensor[slices]\n",
        "    return padded_tensor\n",
        "\n",
        "def compute_node_count_loss(raw_node_counts, true_num_nodes,scale=10000):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        predicted_num_nodes: Scalar - Number of nodes predicted by the model.\n",
        "        true_num_nodes: Scalar - Ground truth number of nodes.\n",
        "    Returns:\n",
        "        node_count_loss: Loss for node count prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    return F.mse_loss(raw_node_counts.float()/scale, true_num_nodes.float()/scale)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_loss(reconstructed_nodes, ground_truth_nodes, reconstructed_adj, ground_truth_adj, scale_node_loss=10.0):\n",
        "\n",
        "    # Scale the node reconstruction loss to increase its weight\n",
        "    loss_nodes = scale_node_loss * F.mse_loss(reconstructed_nodes, ground_truth_nodes)\n",
        "\n",
        "    # Determine the maximum size for padding\n",
        "    max_nodes = max(reconstructed_adj.shape[0], ground_truth_adj.shape[0])\n",
        "\n",
        "    # Pad reconstructed_adj if smaller\n",
        "    if reconstructed_adj.shape[0] < max_nodes:\n",
        "        pad_size = max_nodes - reconstructed_adj.shape[0]\n",
        "        reconstructed_adj = F.pad(reconstructed_adj, (0, pad_size, 0, pad_size), \"constant\", 0)\n",
        "\n",
        "    # Pad ground_truth_adj if smaller\n",
        "    if ground_truth_adj.shape[0] < max_nodes:\n",
        "        pad_size = max_nodes - ground_truth_adj.shape[0]\n",
        "        ground_truth_adj = F.pad(ground_truth_adj, (0, pad_size, 0, pad_size), \"constant\", 0)\n",
        "\n",
        "    # Compute binary cross entropy loss for the adjacency matrix\n",
        "    # Using BCE with logits means you don't need to apply sigmoid on reconstructed_adj first.\n",
        "    loss_adj = F.binary_cross_entropy_with_logits(reconstructed_adj, ground_truth_adj.float())\n",
        "\n",
        "    return loss_nodes, loss_adj\n",
        "\n",
        "\n",
        "def compute_total_loss(reconstructed_nodes, ground_truth_nodes, reconstructed_adj, ground_truth_adj, \n",
        "                      true_num_nodes, predicted_num_nodes, raw_node_counts, z, z_q, \n",
        "                      beta=0.25, w_nodes=1.0, w_count=10.0):\n",
        "    reconstruction_loss = compute_loss(reconstructed_nodes, ground_truth_nodes, reconstructed_adj, ground_truth_adj, \n",
        "                                       true_num_nodes, predicted_num_nodes)\n",
        "    node_count_loss = compute_node_count_loss(raw_node_counts, true_num_nodes, scale=10000)\n",
        "    vq_loss = F.mse_loss(z, z_q.detach())\n",
        "    return w_nodes * reconstruction_loss + w_count * node_count_loss + beta * vq_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def visualize_latent_space(fused_features, method=\"pca\"):\n",
        "    \"\"\"\n",
        "    Visualizes the latent space of fused features using PCA or t-SNE.\n",
        "    \n",
        "    Args:\n",
        "\n",
        "    \n",
        "        fused_features (torch.Tensor): Tensor of shape [num_samples, fused_dim].\n",
        "        method (str): \"pca\" or \"tsne\" to select the dimensionality reduction method.\n",
        "    \"\"\"\n",
        "    # Convert tensor to numpy array\n",
        "    features_np = fused_features.cpu().detach().numpy()\n",
        "\n",
        "    if method == \"pca\":\n",
        "        pca = PCA(n_components=2)\n",
        "        reduced_features = pca.fit_transform(features_np)\n",
        "        plt.figure(figsize=(8,6))\n",
        "        plt.scatter(reduced_features[:, 0], reduced_features[:, 1], alpha=0.7)\n",
        "        plt.title(\"PCA of Fused Features\")\n",
        "        plt.xlabel(\"Principal Component 1\")\n",
        "        plt.ylabel(\"Principal Component 2\")\n",
        "        plt.show()\n",
        "    elif method == \"tsne\":\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        reduced_features = tsne.fit_transform(features_np)\n",
        "        plt.figure(figsize=(8,6))\n",
        "        plt.scatter(reduced_features[:, 0], reduced_features[:, 1], alpha=0.7)\n",
        "        plt.title(\"t-SNE of Fused Features\")\n",
        "        plt.xlabel(\"Dimension 1\")\n",
        "        plt.ylabel(\"Dimension 2\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Method not recognized. Choose either 'pca' or 'tsne'.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_graph(nodes, adj, epoch, save_path=\"visualizations\", display_inline=True):\n",
        "    import matplotlib.pyplot as plt\n",
        "    from mpl_toolkits.mplot3d import Axes3D\n",
        "    import os\n",
        "    import numpy as np\n",
        "\n",
        "    if not os.path.exists(save_path) and not display_inline:\n",
        "        os.makedirs(save_path)\n",
        "    \n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    \n",
        "    if nodes.shape[0] == 0:\n",
        "        print(f\"Warning: No nodes to plot at Epoch {epoch}\")\n",
        "        ax.text(0, 0, 0, \"No nodes generated\")\n",
        "    else:\n",
        "        ax.scatter(nodes[:, 0], nodes[:, 1], nodes[:, 2], c='b', marker='o')\n",
        "        # Use np.argwhere to get a 2D array of edge indices where adj > 0.5.\n",
        "        edges = np.argwhere(adj > 0.5)\n",
        "        if edges.shape[0] == 0:\n",
        "            print(f\"Warning: No edges to plot at Epoch {epoch}\")\n",
        "        else:\n",
        "            for edge in edges:\n",
        "                if edge.size < 2:\n",
        "                    continue\n",
        "                start = nodes[edge[0]]\n",
        "                end = nodes[edge[1]]\n",
        "                ax.plot([start[0], end[0]], [start[1], end[1]], [start[2], end[2]], 'k-')\n",
        "    \n",
        "    ax.set_title(f\"Epoch {epoch}\")\n",
        "    \n",
        "    if display_inline:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.savefig(os.path.join(save_path, f\"epoch_{epoch}.png\"))\n",
        "    \n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# padding and normalizing the dataset,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_node_features(dataset):\n",
        "    all_nodes = torch.cat([data.x for data in dataset], dim=0)\n",
        "    min_val, max_val = all_nodes.min(), all_nodes.max()\n",
        "    for data in dataset:\n",
        "        data.x = 2 * (data.x - min_val) / (max_val - min_val) - 1  # Normalize to [-1, 1]\n",
        "    return dataset\n",
        "\n",
        "# train_dataset = normalize_node_features(train_dataset)\n",
        "# val_dataset = normalize_node_features(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vertices_np, edges_np= train_dataset[1].x.cpu().numpy(), train_dataset[1].edge_index.cpu().numpy()\n",
        "visualize_3d(vertices_np, edges_np)\n",
        "print(train_dataset[1].x.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "def pad_graph(data, max_node_count, node_dim=3):\n",
        "    num_nodes = data.x.size(0)\n",
        "    mask = torch.zeros(max_node_count, dtype=torch.bool, device=data.x.device)\n",
        "    mask[:num_nodes] = 1  # 1 for real nodes, 0 for padded nodes\n",
        "\n",
        "    # Pad node features\n",
        "    padded_x = pad_tensor(data.x, (max_node_count, node_dim), padding_value=0.0)\n",
        "\n",
        "    # Pad adjacency matrix\n",
        "    target_adj = to_dense_adj(data.edge_index).squeeze(0)\n",
        "    padded_adj = pad_tensor(target_adj, (max_node_count, max_node_count), padding_value=0.0)\n",
        "\n",
        "    data.x = padded_x\n",
        "    data.adj = padded_adj\n",
        "    data.mask = mask\n",
        "    return data\n",
        "\n",
        "# Apply padding in the data loader\n",
        "train_loader = DataLoader([pad_graph(d, max_node_count) for d in train_dataset], batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader([pad_graph(d, max_node_count) for d in val_dataset], batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datapoint = pad_graph(train_dataset[1], max_node_count)\n",
        "vertices_np, edges_np = datapoint.x.cpu().numpy(), datapoint.edge_index.cpu().numpy()\n",
        "print(datapoint.x.size())\n",
        "visualize_3d(vertices_np, edges_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing and diagnostics \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "class TrainingDiagnostics:\n",
        "    def __init__(self):\n",
        "        self.gradient_logs = {}\n",
        "        self.feature_ranges = {}\n",
        "        self.loss_logs = {\n",
        "            \"recon_node\": [],\n",
        "            \"recon_adj\": [],\n",
        "            \"vq_loss\": [],\n",
        "            \"total_loss\": [],\n",
        "        }\n",
        "        self.train_curve = []\n",
        "        self.val_curve = []\n",
        "\n",
        "    def track_gradients(self, model):\n",
        "        for name, module in model.items():\n",
        "            for pname, param in module.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    grad_norm = param.grad.norm().item()\n",
        "                    self.gradient_logs[f\"{name}.{pname}\"] = grad_norm\n",
        "                else:\n",
        "                    self.gradient_logs[f\"{name}.{pname}\"] = None\n",
        "\n",
        "    def track_features(self, name, tensor):\n",
        "        if tensor is None: return\n",
        "        stats = {\n",
        "            \"min\": tensor.min().item(),\n",
        "            \"max\": tensor.max().item(),\n",
        "            \"mean\": tensor.mean().item(),\n",
        "        }\n",
        "        if name not in self.feature_ranges:\n",
        "            self.feature_ranges[name] = []\n",
        "        self.feature_ranges[name].append(stats)\n",
        "\n",
        "    def log_losses(self, node_loss, adj_loss, vq_loss, total_loss):\n",
        "        self.loss_logs[\"recon_node\"].append(node_loss)\n",
        "        self.loss_logs[\"recon_adj\"].append(adj_loss)\n",
        "        self.loss_logs[\"vq_loss\"].append(vq_loss)\n",
        "        self.loss_logs[\"total_loss\"].append(total_loss)\n",
        "\n",
        "    def log_epoch_loss(self, train_loss, val_loss):\n",
        "        self.train_curve.append(train_loss)\n",
        "        self.val_curve.append(val_loss)\n",
        "\n",
        "    def summarize(self):\n",
        "        print(\"\\n=== TRAINING SUMMARY ===\")\n",
        "\n",
        "        # Gradient Summary\n",
        "        print(\"\\n[Gradient Flow at Last Step]\")\n",
        "        for name, grad in self.gradient_logs.items():\n",
        "            if grad is None:\n",
        "                print(f\"{name:60s} | grad is None\")\n",
        "            elif grad < 1e-8:\n",
        "                print(f\"{name:60s} | grad norm: {grad:.2e} (vanishing)\")\n",
        "            else:\n",
        "                print(f\"{name:60s} | grad norm: {grad:.6f}\")\n",
        "\n",
        "        # Feature Range Summary\n",
        "        print(\"\\n[Feature Activation Ranges (Last Step)]\")\n",
        "        for name, stats_list in self.feature_ranges.items():\n",
        "            if not stats_list: continue\n",
        "            last = stats_list[-1]\n",
        "            print(f\"{name:25s} | min: {last['min']:.4f}, max: {last['max']:.4f}, mean: {last['mean']:.4f}\")\n",
        "\n",
        "        self.plot_all()\n",
        "\n",
        "    def plot_all(self):\n",
        "        # Loss Component Trends\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for key, values in self.loss_logs.items():\n",
        "            plt.plot(values, label=key)\n",
        "        plt.title(\"Loss Component Trends\")\n",
        "        plt.xlabel(\"Step\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        # Train vs Val Loss\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        plt.plot(self.train_curve, label=\"Train Loss\")\n",
        "        plt.plot(self.val_curve, label=\"Val Loss\")\n",
        "        plt.title(\"Training vs Validation Loss\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "ZomqeACKU5O7",
        "outputId": "5560fa31-4c02-4912-c457-b7adad0890dc"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "\n",
        "def train_one_epoch(model, data_loader, optimizer, device, epoch, diagnostics):\n",
        "    set_mode(model, mode=\"train\")\n",
        "    total_loss = 0\n",
        "    print(\"Training...\",len(data_loader))\n",
        "    \n",
        "    for i, data in enumerate(data_loader):\n",
        "        data = data.to(device)\n",
        "        text_input_ids = data.text_label  \n",
        "        \n",
        "        graph_x = data.x\n",
        "        edge_index = data.edge_index\n",
        "\n",
        "        \n",
        "        c_text = model['text_encoder'](text_input_ids)\n",
        "        z_G = model['graph_encoder'](graph_x, edge_index)\n",
        "        fused_features = model['feature_fusion'](c_text, z_G)\n",
        "        z_qG, _, vq_loss = model['vector_quantizer'](fused_features)\n",
        "        true_num_nodes = torch.tensor(graph_x.size(0), device=device, dtype=torch.long)\n",
        "        reconstructed_nodes, reconstructed_adj = model['graph_decoder'](z_qG, true_num_nodes)\n",
        "        \n",
        "        target_adj = to_dense_adj(edge_index).squeeze(0)\n",
        "        loss_nodes , loss_adj = compute_loss(reconstructed_nodes, graph_x, reconstructed_adj, target_adj)\n",
        "        loss = 10 * loss_nodes + loss_adj + 10* vq_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        \n",
        "        for name, module in model.items():\n",
        "            factor = 10.0 if name in ['text_encoder', 'graph_encoder', 'vector_quantizer'] else 5.0\n",
        "            for param in module.parameters():\n",
        "                if param.grad is not None:\n",
        "                    param.grad *= factor\n",
        "        \n",
        "        # Optionally, clip gradients after scaling\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        # Before optimizer.step()\n",
        "        diagnostics.track_gradients(model)\n",
        "        diagnostics.track_features(\"Decoder Nodes\", reconstructed_nodes)\n",
        "        diagnostics.track_features(\"Decoder Adj\", reconstructed_adj)\n",
        "        diagnostics.log_losses(\n",
        "            loss_nodes.item(),  # node loss\n",
        "            loss_adj.item(),\n",
        "            vq_loss.item(),\n",
        "            loss.item()\n",
        "        )\n",
        "\n",
        "        # print(i)\n",
        "        # if i == len(data_loader) - 1 or i == 1:\n",
        "        #     print(reconstructed_adj)\n",
        "        #     print(f\"Epoch {epoch+1}, Batch 1:\")\n",
        "        #     print(f\"  reconstructed_nodes shape: {reconstructed_nodes.shape}\")\n",
        "        #     print(f\"  reconstructed_nodes min: {reconstructed_nodes.min().item():.4f}, max: {reconstructed_nodes.max().item():.4f}\")\n",
        "        #     print(f\"  reconstructed_adj shape: {reconstructed_adj.shape}\")\n",
        "        #     print(f\"  reconstructed_adj min: {reconstructed_adj.min().item():.4f}, max: {reconstructed_adj.max().item():.4f}\")\n",
        "        #     print(f\"  Node Reconstruction Loss: {10*loss_nodes.item():.4f} adj Reconstruction Loss: {loss_adj.item():.4f}\")\n",
        "        #     print(f\"  VQ Loss: {100*vq_loss.item():.4f}\")\n",
        "        #     print(f\"  Total Loss: {loss.item():.4f}\")\n",
        "\n",
        "        #     print(\"Decoder\")\n",
        "        #     for name, param in model['graph_decoder'].named_parameters():\n",
        "        #         if param.grad is not None:\n",
        "        #             print(f\"  Grad {name}: {param.grad.abs().mean().item():.6f}\")\n",
        "        #         else:\n",
        "        #             print(f\"  Grad {name}: None\")\n",
        "\n",
        "        #     print(\"vector_quantizer\")\n",
        "        #     for name, param in model['vector_quantizer'].named_parameters():\n",
        "        #         if param.grad is not None:\n",
        "        #             print(f\"  Grad {name}: {param.grad.abs().mean().item():.6f}\")\n",
        "        #         else:\n",
        "        #             print(f\"  Grad {name}: None\")\n",
        "        #     print(\"FeatureFusion\")\n",
        "        #     for name, param in model['feature_fusion'].named_parameters():\n",
        "        #         if param.grad is not None:\n",
        "        #             print(f\"  Grad {name}: {param.grad.abs().mean().item():.6f}\")\n",
        "        #         else:\n",
        "        #             print(f\"  Grad {name}: None\")\n",
        "        #     print(\"graph_encoder\")\n",
        "        #     for name, param in model['graph_encoder'].named_parameters():\n",
        "        #         if param.grad is not None:\n",
        "        #             print(f\"  Grad {name}: {param.grad.abs().mean().item():.6f}\")\n",
        "        #         else:\n",
        "        #             print(f\"  Grad {name}: None\")\n",
        "        #     print(\"text_encoder\")\n",
        "        #     for name, param in model['text_encoder'].named_parameters():\n",
        "        #         if param.grad is not None:\n",
        "        #             print(f\"  Grad {name}: {param.grad.abs().mean().item():.6f}\")\n",
        "        #         else:\n",
        "        #             print(f\"  Grad {name}: None\")\n",
        "                \n",
        "        #     # Uncomment to visualize\n",
        "        #     nodes = reconstructed_nodes.cpu().detach().numpy()\n",
        "        #     adj = torch.sigmoid(reconstructed_adj).cpu().detach().numpy()\n",
        "        #     visualize_graph(nodes, adj, epoch)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()   \n",
        "    \n",
        "    return total_loss / len(data_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    set_mode(model, mode=\"eval\")\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "            text_input_ids = data.text_label  # shape: [batch_size, seq_len]\n",
        "        # Optionally, if available:\n",
        "           \n",
        "            graph_x = data.x\n",
        "            edge_index = data.edge_index\n",
        "\n",
        "        # Pass tokenized text through the BertTextEncoder\n",
        "            c_text = model['text_encoder'](text_input_ids)\n",
        "            z_G = model['graph_encoder'](graph_x, edge_index)\n",
        "            fused_features = model['feature_fusion'](c_text, z_G)\n",
        "            z_qG, _, vq_loss = model['vector_quantizer'](fused_features)\n",
        "            true_num_nodes = torch.tensor(graph_x.size(0), device=device, dtype=torch.long)  # Convert to tensor\n",
        "            reconstructed_nodes, reconstructed_adj = model['graph_decoder'](z_qG,true_num_nodes)\n",
        "\n",
        "            target_adj = to_dense_adj(edge_index).squeeze(0)\n",
        "\n",
        "            loss_nodes , loss_adj = compute_loss(reconstructed_nodes, graph_x, reconstructed_adj, target_adj)\n",
        "            vq_loss = F.mse_loss(fused_features, z_qG.detach())  # Vector quantization loss\n",
        "            loss = 1 * loss_nodes + loss_adj + 0.25 * vq_loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def infer_and_decode(model, data, device):\n",
        "    set_mode(model, mode=\"eval\")  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        data = data.to(device)\n",
        "        text_input = data.text_label\n",
        "        graph_x = data.x\n",
        "        edge_index = data.edge_index\n",
        "\n",
        "        # Encode\n",
        "        c_text = model['text_encoder'](text_input)\n",
        "        z_G = model['graph_encoder'](graph_x, edge_index)\n",
        "        fused_features = model['feature_fusion'](c_text, z_G)\n",
        "        z_qG, _, vq_loss = model['vector_quantizer'](fused_features)\n",
        "        true_num_nodes = torch.tensor(graph_x.size(0), device=device, dtype=torch.long)  # Convert to tensor\n",
        "        reconstructed_nodes, reconstructed_adj = model['graph_decoder'](z_qG,true_num_nodes)\n",
        "        \n",
        "        return reconstructed_nodes.cpu().numpy(), reconstructed_adj.cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "def main_training(num_epochs=40, batch_size=1, learning_rate=1e-3, patience=2):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    optimizer = Adam([\n",
        "        {'params': model['text_encoder'].parameters(), 'lr': 2e-3},\n",
        "        {'params': model['graph_encoder'].parameters(), 'lr': 2e-3},\n",
        "        {'params': model['feature_fusion'].parameters(), 'lr': 1e-3},\n",
        "        {'params': model['vector_quantizer'].parameters(), 'lr': 1e-2},\n",
        "        {'params': model['graph_decoder'].parameters(), 'lr':2e-3}\n",
        "    ])\n",
        "    \n",
        "    diagnostics = TrainingDiagnostics()\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, device, epoch,diagnostics)\n",
        "        val_loss = evaluate(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "        diagnostics.log_epoch_loss(train_loss, val_loss)\n",
        "        \n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            torch.save({name: part.state_dict() for name, part in model.items()}, \"best_model.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                diagnostics.summarize()\n",
        "                break\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            nodes, adj = infer_and_decode(model, data, device)\n",
        "            visualize_graph(nodes, adj, epoch)\n",
        "            break\n",
        "    checkpoint = torch.load(\"best_model.pth\")\n",
        "    for name, part in model.items():\n",
        "        part.load_state_dict(checkpoint[name])\n",
        "    test_loss = evaluate(model, DataLoader(test_dataset, batch_size=batch_size), device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "main_training()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_lstm_feature_fusion(model, data_loader, optimizer, device, num_epochs=10):\n",
        "    model['lstm_feature_fusion'].train()\n",
        "    model['text_encoder'].eval()\n",
        "    model['graph_encoder'].eval()\n",
        "    model['feature_fusion'].eval()\n",
        "    total_loss = 0\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        latent_features_accumulated = [] \n",
        "        for data in data_loader:\n",
        "            data = data.to(device)\n",
        "\n",
        "            # Generate Ground Truth from Pretrained Feature Fusion\n",
        "            text_embeddings = model['text_encoder'](data.text_label.unsqueeze(1))  # [batch_size, seq_len, embed_dim]\n",
        "            z_G = model['graph_encoder'](data.x, data.edge_index)  # Graph features\n",
        "\n",
        "            # Ground Truth Feature Fusion Output\n",
        "            fused_features = model['feature_fusion'](text_embeddings, z_G)  # [batch_size, fused_dim]\n",
        "            latent_features_accumulated.append(fused_features)\n",
        "            # LSTM Prediction\n",
        "            lstm_pred = model['lstm_feature_fusion'](text_embeddings)  # [batch_size, fused_dim]\n",
        "            # Compute Loss\n",
        "            loss = loss_fn(lstm_pred, fused_features)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        total_loss = epoch_loss / len(data_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - LSTM Loss: {total_loss:.4f}\")\n",
        "\n",
        "    print(\"LSTM Training Complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Main Training Loop\n",
        "def main_training(num_epochs=40, batch_size=1, learning_rate=1e-4):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = Adam(\n",
        "        [param for part in model.values() for param in part.parameters()],\n",
        "        lr=learning_rate,\n",
        "    )\n",
        "\n",
        "    # Load Data\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  \n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Training Loop\n",
        "    train_lstm_feature_fusion(model, train_loader, optimizer, device,num_epochs)\n",
        "        \n",
        "    # print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    print(\"Training Complete!\")\n",
        "\n",
        "    # Testing\n",
        "    test_loss = evaluate(model, test_loader, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    \n",
        "# Run the training\n",
        "main_training()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNudV8IXXnku"
      },
      "outputs": [],
      "source": [
        "torch.save({name: part.state_dict() for name, part in model.items()}, \"trained_model.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_feature_fusion_from_text(model, text_input, device):\n",
        "    \"\"\"\n",
        "    Generate feature fusion output using only text embeddings.\n",
        "\n",
        "    Args:\n",
        "        model (dict): Trained model.\n",
        "        text_input (Tensor): Tokenized text input [batch_size, seq_len, embed_dim].\n",
        "        device: Torch device (cuda/cpu).\n",
        "\n",
        "    Returns:\n",
        "        Predicted feature fusion output [batch_size, fused_dim].\n",
        "    \"\"\"\n",
        "    model['lstm_feature_fusion'].eval()\n",
        "    model['text_encoder'].eval()\n",
        "    model['graph_encoder'].eval()\n",
        "    model['feature_fusion'].eval()\n",
        "    with torch.no_grad():\n",
        "        text_embeddings = model['text_encoder'](text_input.unsqueeze(1))  # [batch_size, seq_len, embed_dim]\n",
        "        predicted_fused_features = model['lstm_feature_fusion'](text_embeddings)  # [batch_size, fused_dim]\n",
        "    \n",
        "    return predicted_fused_features\n",
        "\n",
        "inputs = tokenizer(\"The 3D model features a central fuselage with two horizontal wings mounted symmetrically above, and a single vertical stabilizer at the rear. It includes a circular propeller at the front and two elongated landing gear elements beneath, creating a compact aircraft shape.\", return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
        "input_ids = inputs[\"input_ids\"].to(dtype=torch.long)  # Ensure integer token IDs and move to device\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = bert_model(input_ids.to(device))  # Ensure input is on the correct device\n",
        "text_label_embedding = outputs.last_hidden_state.to(device)  # Pool the embeddings and move to device\n",
        "print(text_label_embedding.shape)\n",
        "pooled_text_label = text_label_embedding.mean(dim=1)\n",
        "ff=generate_feature_fusion_from_text(model,pooled_text_label,device)\n",
        "print(ff.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def generate_graph_from_bert(model, bert_embedding, device):\n",
        "#     \"\"\"\n",
        "#     Generate a graph using BERT embeddings and the codebook.\n",
        "    \n",
        "#     Args:\n",
        "#         model: Dictionary containing the model components.\n",
        "#         bert_embedding: Tensor of shape [1, embedding_dim] from BERT.\n",
        "#         device: Device to run the model on (CPU or GPU).\n",
        "#     \"\"\"\n",
        "#     # Move BERT embedding to the correct device\n",
        "#     bert_embedding = bert_embedding.to(device)  # Shape: [1, embedding_dim]\n",
        "\n",
        "#     # Quantize the BERT embedding\n",
        "#     with torch.no_grad():\n",
        "#         z_q, _ = model['vector_quantizer'](bert_embedding)  # Quantized representation\n",
        "\n",
        "#     # Decode the quantized embeddings to generate graph\n",
        "#     reconstructed_nodes, reconstructed_adj = model['graph_decoder'](z_q)\n",
        "#     print(\"Reconstructed Nodes:\", reconstructed_nodes)\n",
        "#     print(\"Reconstructed Adjacency Matrix:\", reconstructed_adj)\n",
        "\n",
        "#     # Visualize the generated graph\n",
        "#     visualize_3d(reconstructed_nodes, reconstructed_adj)\n",
        "\n",
        "# # Example Usage\n",
        "# generate_graph_from_bert(model, text_label_embedding, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model['vector_quantizer'].eval()\n",
        "    model['graph_decoder'].eval()\n",
        "    z_latent,_ = model['vector_quantizer'](ff)\n",
        "    print(z_latent.size)\n",
        "    new_nodes, new_adj = model['graph_decoder'](z_latent,2479)    \n",
        "    print(\"Generated Nodes:\", new_nodes.shape)\n",
        "    print(\"Generated Adjacency Matrix:\", new_adj.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def adjacency_matrix_to_edge_list(adj_matrix):\n",
        "    \"\"\"\n",
        "    Convert an adjacency matrix to an edge list.\n",
        "\n",
        "    Args:\n",
        "        adj_matrix (torch.Tensor or np.ndarray): Adjacency matrix of shape [num_nodes, num_nodes].\n",
        "\n",
        "    Returns:\n",
        "        edge_list (np.ndarray): Edge list of shape [2, num_edges].\n",
        "    \"\"\"\n",
        "    adj_matrix = adj_matrix.cpu().numpy() if isinstance(adj_matrix, torch.Tensor) else adj_matrix\n",
        "    edge_list = np.array(np.nonzero(adj_matrix))  # Extract non-zero entries (edges)\n",
        "    return edge_list  # Shape: [2, num_edges]\n",
        "\n",
        "# Convert adjacency matrix to edge list\n",
        "edge_list = adjacency_matrix_to_edge_list(new_adj)\n",
        "\n",
        "# Visualize the graph\n",
        "visualize_3d(new_nodes.cpu().numpy(), edge_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_3d(vertices_np, edges_np)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cd779ddf0be49bb857bbee186a8bdb5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f0958f6189b4948926a236c26eb8c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1093834e1ad84e52adc3155a979bc1a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ebf19089e0f460aaa211c6ff1996b02",
            "placeholder": "​",
            "style": "IPY_MODEL_bab982ff160a4d73b993ed5db2c49218",
            "value": "model.safetensors: 100%"
          }
        },
        "134d780ed6c64778a4ce565339b61686": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2397f1d2773d46ba8a3f408208293888": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24027343e539405a95bb88f4cf0d5044": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "297b8c9d245f42a4864d853f0674aae6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29cb580877af43e586f4d76c288a6380": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e6617cbf36c4e91973b888cd244bf2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e9b5736d2414acda9d15ac96473c885": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "337eb33f480440a48149ce9c1a1aa4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36355540013a4f78be3ca41e44b6926d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37ade2c5ac3e4fd38a02200c84bac548": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "382bc7f77d0d47f9ae4bbb00a40128e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c8fafad03b49b993b1569ca396197b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5971a7ddd8594a5cb9ec060168bc14ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bbb1a78cf5b49feb35d9f1a3a55a8db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d15e1ab66e147eeb027ebf9d86bc435": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ee3cfc2d2a64a5286907d9d1e1c5a13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "607db37930064289878b1791b8ac7bd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f0958f6189b4948926a236c26eb8c5f",
            "placeholder": "​",
            "style": "IPY_MODEL_337eb33f480440a48149ce9c1a1aa4a6",
            "value": " 466k/466k [00:00&lt;00:00, 13.8MB/s]"
          }
        },
        "6cbde44d11334b5b944c8410c8cb0d38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ebf19089e0f460aaa211c6ff1996b02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ec9249ba6364b8eb9c8cdee07d62f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37ade2c5ac3e4fd38a02200c84bac548",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8efdfafa7594d648715b7388115b6bc",
            "value": 48
          }
        },
        "7bb00053b88d47d684ded22fc79b7061": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab44d06347094ed5bc15e5ee232eac5f",
            "placeholder": "​",
            "style": "IPY_MODEL_c2dcb84419e84f4c94b062e256c60fc7",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8002c62e95af4128b45b4759fd1ae9a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9a0986b39f6486f8a1ebcc597d744ba",
            "placeholder": "​",
            "style": "IPY_MODEL_2e6617cbf36c4e91973b888cd244bf2b",
            "value": " 440M/440M [00:03&lt;00:00, 185MB/s]"
          }
        },
        "90e0bb2fe52e4080be9913569e83c314": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cbde44d11334b5b944c8410c8cb0d38",
            "placeholder": "​",
            "style": "IPY_MODEL_f43b792d1ac142b4b29d3a6081b89646",
            "value": "tokenizer.json: 100%"
          }
        },
        "90fe37851c7a47138a7006eb54fcdd4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97bdf1a4b7424908b96bd0454cd3a708": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "97c440d79b63422bb1653f7f06a1a079": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b21fdc01b76b484f88ebff488171588a",
              "IPY_MODEL_d56aa33f2cf843109608aad4a243c294",
              "IPY_MODEL_c7ca3809b88b4a0b8668897452dbda7c"
            ],
            "layout": "IPY_MODEL_2e9b5736d2414acda9d15ac96473c885"
          }
        },
        "97c786a45441467b8d874fe5c25c6126": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc7945f033b24db089f116da7ac98aeb",
              "IPY_MODEL_9892b898f51540098e2fc72b6a41ebce",
              "IPY_MODEL_d68069b8111e47b69365b3e3760d1edf"
            ],
            "layout": "IPY_MODEL_29cb580877af43e586f4d76c288a6380"
          }
        },
        "97e43e3f2ff4430589ce42ac608b790c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1093834e1ad84e52adc3155a979bc1a8",
              "IPY_MODEL_9db512326f264814aa7c1172dc6c8ea3",
              "IPY_MODEL_8002c62e95af4128b45b4759fd1ae9a7"
            ],
            "layout": "IPY_MODEL_9de36e3c313a4a789a849204162e18dc"
          }
        },
        "9892b898f51540098e2fc72b6a41ebce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36355540013a4f78be3ca41e44b6926d",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2a056a85c984797b34fa0fdb45a1298",
            "value": 570
          }
        },
        "9db512326f264814aa7c1172dc6c8ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_382bc7f77d0d47f9ae4bbb00a40128e5",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5bbb1a78cf5b49feb35d9f1a3a55a8db",
            "value": 440449768
          }
        },
        "9de36e3c313a4a789a849204162e18dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a056a85c984797b34fa0fdb45a1298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab44d06347094ed5bc15e5ee232eac5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0658da51e9645af95ce49e50281cc6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b21fdc01b76b484f88ebff488171588a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c8fafad03b49b993b1569ca396197b",
            "placeholder": "​",
            "style": "IPY_MODEL_b628ae7a8f6544c286311ac4cae11eed",
            "value": "vocab.txt: 100%"
          }
        },
        "b628ae7a8f6544c286311ac4cae11eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9b1310b2cc844b58d36f03ce2121df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ee3cfc2d2a64a5286907d9d1e1c5a13",
            "placeholder": "​",
            "style": "IPY_MODEL_2397f1d2773d46ba8a3f408208293888",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.15kB/s]"
          }
        },
        "bab982ff160a4d73b993ed5db2c49218": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd21d9d2bfed40d8887ff97cbb571205": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2dcb84419e84f4c94b062e256c60fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7ca3809b88b4a0b8668897452dbda7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d230aec0b9c540cc892ecd07012c404e",
            "placeholder": "​",
            "style": "IPY_MODEL_134d780ed6c64778a4ce565339b61686",
            "value": " 232k/232k [00:00&lt;00:00, 5.08MB/s]"
          }
        },
        "cc7945f033b24db089f116da7ac98aeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d15e1ab66e147eeb027ebf9d86bc435",
            "placeholder": "​",
            "style": "IPY_MODEL_f1b416f44ddd4b4c9df5eef75978c6f4",
            "value": "config.json: 100%"
          }
        },
        "d230aec0b9c540cc892ecd07012c404e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d56aa33f2cf843109608aad4a243c294": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd21d9d2bfed40d8887ff97cbb571205",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97bdf1a4b7424908b96bd0454cd3a708",
            "value": 231508
          }
        },
        "d68069b8111e47b69365b3e3760d1edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_297b8c9d245f42a4864d853f0674aae6",
            "placeholder": "​",
            "style": "IPY_MODEL_24027343e539405a95bb88f4cf0d5044",
            "value": " 570/570 [00:00&lt;00:00, 14.1kB/s]"
          }
        },
        "dbad3e0bf4104835abeb6966d9865d66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90fe37851c7a47138a7006eb54fcdd4d",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5971a7ddd8594a5cb9ec060168bc14ac",
            "value": 466062
          }
        },
        "e688fbce31824171994232209d5c43c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90e0bb2fe52e4080be9913569e83c314",
              "IPY_MODEL_dbad3e0bf4104835abeb6966d9865d66",
              "IPY_MODEL_607db37930064289878b1791b8ac7bd4"
            ],
            "layout": "IPY_MODEL_b0658da51e9645af95ce49e50281cc6d"
          }
        },
        "e8efdfafa7594d648715b7388115b6bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f14f5a8c778a4393933842e6737cb631": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7bb00053b88d47d684ded22fc79b7061",
              "IPY_MODEL_6ec9249ba6364b8eb9c8cdee07d62f28",
              "IPY_MODEL_b9b1310b2cc844b58d36f03ce2121df6"
            ],
            "layout": "IPY_MODEL_0cd779ddf0be49bb857bbee186a8bdb5"
          }
        },
        "f1b416f44ddd4b4c9df5eef75978c6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f43b792d1ac142b4b29d3a6081b89646": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9a0986b39f6486f8a1ebcc597d744ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
